package llm

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log"
	"strings"

	"sync"
	"time"

	"tts-book/backend/internal/config"

	"github.com/sashabaranov/go-openai"
	"google.golang.org/genai"
)

const systemPrompt = `
	åˆ†ææä¾›çš„æ–‡æœ¬ï¼Œå¹¶ä¸¥æ ¼å°†å…¶åˆ†å‰²ä¸º JSON å¯¹è±¡åˆ—è¡¨ã€‚
	å¿…é¡»åŒ…å«è¾“å…¥ä¸­çš„ã€æ‰€æœ‰æ–‡æœ¬ã€‘ï¼Œå¹¶ä¿æŒã€åŸå§‹é¡ºåºã€‘ã€‚
	è¿”å›å¿…é¡»æ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å« "segments" æ•°ç»„ã€‚

	å…³é”®è§„åˆ™ 1ï¼šã€å¼ºåˆ¶ã€‘åˆ†å‰²å¯¹è¯ä¸æ—ç™½ (MANDATORY Split)
	æ ¸å¿ƒåŸåˆ™ï¼š**ä¸¥ç¦**åœ¨ä¸€ä¸ª JSON å¯¹è±¡ä¸­åŒæ—¶åŒ…å«å¼•å·å†…çš„å†…å®¹ï¼ˆå¯¹è¯ï¼‰å’Œå¼•å·å¤–çš„å†…å®¹ï¼ˆæ—ç™½ï¼‰ã€‚
	
	æ‰§è¡Œæ­¥éª¤ï¼š
	1. æ‰«ææ–‡æœ¬ï¼Œæ‰¾åˆ°æ‰€æœ‰çš„å¼•å·ï¼ˆâ€œ...â€ æˆ– "..."ï¼‰ã€‚
	2. å°†å¼•å·å†…çš„éƒ¨åˆ†æå–ä¸º { "speaker": "è§’è‰²å", ... }ã€‚
	3. å°†å¼•å·å¤–çš„éƒ¨åˆ†ï¼ˆåŒ…æ‹¬æè¿°ã€åŠ¨ä½œã€æ ‡ç‚¹ï¼‰æå–ä¸º { "speaker": "Narrator", ... }ã€‚
	4. ä¿æŒåŸæ–‡çš„ç‰©ç†é¡ºåºã€‚

	å¸¸è§ç»“æ„å¤„ç†ï¼š
	1. [åŠ¨ä½œ, å¯¹è¯]ï¼š
	   åŸæ–‡ï¼šä»–æ‘¸äº†æ‘¸å¥¹çš„è„¸ï¼Œâ€œä½ ä¸è¯¥æŒ‘èµ·è¿™å‰¯é‡æ‹…ï¼Œä½†ä½ å¼Ÿå¼Ÿå¤ªå°ã€‚â€
	   æ‹†åˆ†ï¼š
	   - {"text": "ä»–æ‘¸äº†æ‘¸å¥¹çš„è„¸ï¼Œ", "speaker": "Narrator", ...}
	   - {"text": "â€œä½ ä¸è¯¥æŒ‘èµ·è¿™å‰¯é‡æ‹…ï¼Œä½†ä½ å¼Ÿå¼Ÿå¤ªå°ã€‚â€", "speaker": "åŠ ä¼¯Â·è’™æ´›å¡æ‰˜", ...}
	   æ³¨æ„ï¼šã€ï¼Œã€‘å½’å±æ—ç™½ã€‚

	2. [å¯¹è¯, åŠ¨ä½œ]ï¼š
	   åŸæ–‡ï¼šâ€œå¿«è·‘ï¼â€ä»–å¤§å–Šã€‚
	   æ‹†åˆ†ï¼š
	   - {"text": "â€œå¿«è·‘ï¼â€", "speaker": "è§’è‰²å", ...}
	   - {"text": "ä»–å¤§å–Šã€‚", "speaker": "Narrator", ...}

	3. [å¯¹è¯, åŠ¨ä½œ, å¯¹è¯] (ä¸‰æ˜æ²»ç»“æ„)ï¼š
	   åŸæ–‡ï¼šâ€œè’™æ‰ï¼Œâ€ä»–ä¼šç¬‘çœ¯çœ¯åœ°ä¿¯è§†å¥¹ï¼Œâ€œæ²¡æœ‰ä½ æˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿâ€
	   æ‹†åˆ†ï¼š
	   - {"text": "â€œè’™æ‰ï¼Œâ€", "speaker": "åŠ ä¼¯", ...}
	   - {"text": "ä»–ä¼šç¬‘çœ¯çœ¯åœ°ä¿¯è§†å¥¹ï¼Œ", "speaker": "Narrator", ...}
	   - {"text": "â€œæ²¡æœ‰ä½ æˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿâ€", "speaker": "åŠ ä¼¯", ...}

	4. [å¤æ‚äº¤æ›¿] (Complex Interleaved):
	   åŸæ–‡ï¼šå¥¹å¹äº†å£æ°”ï¼Œâ€œäº‹å®å°±æ˜¯äº‹å®ã€‚â€å¥¹åœ¨é©¬éä¸Šä¼¸ä¸ªæ‡’è…°ï¼Œâ€œä¸è¿‡ï¼Œæˆ‘çˆ±å¬ã€‚â€
	   æ‹†åˆ†ï¼š
	   - {"text": "å¥¹å¹äº†å£æ°”ï¼Œ", "speaker": "Narrator"} (åŠ¨ä½œæŒ‡ç¤ºä¸»ä½“)
	   - {"text": "â€œäº‹å®å°±æ˜¯äº‹å®ã€‚â€", "speaker": "å¥¹(è§’è‰²å)", "emotion": "melancholic"} (ç”±å¹æ°”æ¨æ–­)
	   - {"text": "å¥¹åœ¨é©¬éä¸Šä¼¸ä¸ªæ‡’è…°ï¼Œ", "speaker": "Narrator"}
	   - {"text": "â€œä¸è¿‡ï¼Œæˆ‘çˆ±å¬ã€‚â€", "speaker": "å¥¹(è§’è‰²å)", "emotion": "calm"} (ç”±ä¼¸æ‡’è…°æ¢å¤å¹³é™)

	- æ—ç™½ (Narrator)ï¼šæè¿°åŠ¨ä½œã€åœºæ™¯ã€‚Speaker: 'Narrator'ã€‚
	- å¯¹è¯ (Dialogue)ï¼šå¼•å·å†…çš„å†…å®¹ã€‚Speaker: è§’è‰²åç§°ã€‚

	å…³é”®è§„åˆ™ 2ï¼šTypesetting å­—æ®µ (Pinyin Annotation)
	"typesetting" å­—æ®µä¸“é—¨ç”¨äºç»™ TTS å¼•æ“æä¾›æ ‡å‡†å‘éŸ³ã€‚
	1. ã€é»˜è®¤è¡Œä¸ºã€‘ï¼šå®Œå…¨å¤åˆ¶ "text" å­—æ®µçš„å†…å®¹ã€‚
	2. ã€ä»…ä¿®æ”¹å¤šéŸ³å­—ã€‘ï¼šé‡åˆ°ä»¥ä¸‹åˆ—è¡¨ä¸­çš„å¤šéŸ³å­—æ—¶ï¼Œã€ç»å¯¹ç¦æ­¢ã€‘åœ¨ typesetting ä¸­ä¿ç•™è¯¥æ±‰å­—æœ¬èº«ã€‚ä½ å¿…é¡»æŠŠé‚£ä¸ªæ±‰å­—ã€åˆ æ‰ã€‘ï¼Œå¹¶åœ¨å…¶åŸä½ç½®å†™ä¸Šå¤§å†™æ‹¼éŸ³å’Œå£°è°ƒæ•°å­—ã€‚
	3. ã€ä¸Šä¸‹æ–‡è¯­å¢ƒåˆ†æã€‘ï¼šå¿…é¡»æ ¹æ®å½“å‰è¿™å¥è¯åœ¨æ•´ä¸ªå‰§æƒ…ä¸­çš„è¯­å¢ƒã€äººç‰©èº«ä»½ã€åŠ¨ä½œæ¥åˆ¤æ–­å¤šéŸ³å­—çš„æ­£ç¡®è¯»éŸ³ã€‚ä¾‹å¦‚ï¼Œâ€œä»–é‡é‡åœ°æ‘”åœ¨åœ°ä¸Šâ€ï¼ˆZHONG4 ZHONG4 DE5ï¼‰ã€‚

	ğŸš¨ğŸš¨ğŸš¨ æå…¶ä¸¥æ ¼çš„æ ¼å¼è­¦å‘Š ğŸš¨ğŸš¨ğŸš¨
	ä¸¥ç¦å‡ºç°â€œåŸå­—+æ‹¼éŸ³â€çš„ç»„åˆï¼
	ã€æ­£ç¡®ç¤ºä¾‹ã€‘ï¼š æŠŠ "éš¾äº§" å˜æˆ "NAN2äº§"
	ã€é”™è¯¯ç¤ºä¾‹ 1 (åŒ…å«åŸå­—) ã€‘ï¼š æŠŠ "éš¾äº§" å˜æˆ "éš¾NAN2äº§" (å¯¼è‡´TTSè¯»é”™)
	ã€é”™è¯¯ç¤ºä¾‹ 2 (åå¼ƒå­—) ã€‘ï¼š æŠŠ "éš¾äº§" å˜æˆ "NAN2"

	å¤šéŸ³å­—å¼ºåˆ¶æ›¿æ¢åˆ—è¡¨ (Mandatory Pinyin Replacement):
	- ã€è¡Œã€‘ï¼šHANG2 (é“¶è¡Œè¡Œé•¿, è¡Œä¸š) / XING2 (è¡Œä¸º, è¡Œèµ°)
	- ã€å¾—ã€‘ï¼šDEI3 (å¾—å») / DE2 (è·‘å¾—å¿«) / DE5 (è§‰å¾—)
	- ã€åœ°ã€‘ï¼šDI4 (ç”°åœ°) / DE5 (æ…¢æ…¢åœ°)
	- ã€é‡ã€‘ï¼šCHONG2 (é‡æ–°, é‡å¤) / ZHONG4 (é‡è¦, é‡æ‹…, é‡é‡åœ°)
	- ã€ç€ã€‘ï¼šZHAO2 (ç€ç«, ç¡ç€) / ZHE5 (çœ‹ç€, èµ°ç€) / ZHUO2 (ç€è£…)
	- ã€é•¿ã€‘ï¼šCHANG2 (é•¿çŸ­, é•¿æª) / ZHANG3 (é•¿å¤§, é•¿å®˜)
	- ã€ä¹ã€‘ï¼šLE4 (å¿«ä¹) / YUE4 (éŸ³ä¹)
	- ã€å¥½ã€‘ï¼šHAO3 (å¥½äºº, å¥½åƒ) / HAO4 (çˆ±å¥½, å¥½å¤§å–œåŠŸ)
	- ã€å¹²ã€‘ï¼šGAN1 (å¹²å‡€, é¥¼å¹²) / GAN4 (å¹²æ´», èƒ½å¹²)
	- ã€éš¾ã€‘ï¼šNAN2 (éš¾äº§, å›°éš¾, ä¸ºéš¾) / NAN4 (ç¾éš¾, éš¾æ°‘)
	- ã€é™ã€‘ï¼šJIANG4 (é™è½, ä¸‹é™) / XIANG2 (æŠ•é™, é™æœ)
	- ã€ä¼ ã€‘ï¼šCHUAN2 (ä¼ è¯´, ä¼ é€’) / ZHUAN4 (ä¼ è®°)

	å…³é”®è§„åˆ™ 3ï¼šç²¾å‡†è¯†åˆ«è§’è‰² (Contextual Speaker Inference)
	æ ¹æ®ä¸Šä¸‹æ–‡æ¨ç†è§’è‰²åç§°ï¼Œä¸¥ç¦ä½¿ç”¨â€œç”·è§’è‰²â€ã€â€œå¥³è§’è‰²â€ã€‚

	ç¤ºä¾‹ï¼š
	è¾“å…¥ï¼š
	ä»–æ‘¸äº†æ‘¸å¥¹çš„è„¸ï¼Œâ€œä½ ä¸è¯¥æŒ‘èµ·è¿™å‰¯é‡æ‹…ï¼Œä½†ä½ å¼Ÿå¼Ÿå¤ªå°ã€‚â€

	è¾“å‡ºï¼š
	{
	  "segments": [
		{
		  "text": "ä»–æ‘¸äº†æ‘¸å¥¹çš„è„¸ï¼Œ",
		  "typesetting": "ä»–æ‘¸äº†æ‘¸å¥¹çš„è„¸ï¼Œ",
		  "speaker": "Narrator",
		  "emotion": "calm"
		},
		{
		  "text": "â€œä½ ä¸è¯¥æŒ‘èµ·è¿™å‰¯é‡æ‹…ï¼Œä½†ä½ å¼Ÿå¼Ÿå¤ªå°ã€‚â€",
		  "typesetting": "â€œä½ ä¸è¯¥æŒ‘èµ·è¿™å‰¯ZHONG4æ‹…ï¼Œä½†ä½ å¼Ÿå¼Ÿå¤ªå°ã€‚â€",
		  "speaker": "åŠ ä¼¯Â·è’™æ´›å¡æ‰˜",
		  "emotion": "sad"
		}
	  ]
	}

	Emotion å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š[happy, angry, sad, afraid, disgusted, melancholic, surprised, calm]ã€‚
	é»˜è®¤ä¸º 'calm'ã€‚
	ä»…è¿”å›ä¸¥æ ¼æœ‰æ•ˆçš„ JSONã€‚`

type AnalysisResult struct {
	Text        string `json:"text"`
	Typesetting string `json:"typesetting,omitempty"` // Text with Pinyin annotations for TTS
	Speaker     string `json:"speaker"`
	Emotion     string `json:"emotion"`
}

type Client struct {
	api             *openai.Client
	genaiClient     *genai.Client
	mu              sync.Mutex
	lastRequestTime time.Time
	minInterval     time.Duration
	isMock          bool
	model           string
	provider        string
	apiKey          string
}

func NewClient(cfg *config.Config) *Client {
	c := openai.DefaultConfig(cfg.LLMAPIKey)
	c.BaseURL = cfg.LLMBaseURL

	client := &Client{
		api:         openai.NewClientWithConfig(c),
		minInterval: time.Duration(cfg.LLMMinInterval) * time.Millisecond,
		isMock:      cfg.MockLLM,
		provider:    cfg.LLMProvider,
		apiKey:      cfg.LLMAPIKey,
		model:       cfg.LLMModel,
	}

	if cfg.LLMProvider == "gemini" {
		ctx := context.Background()
		gClient, err := genai.NewClient(ctx, &genai.ClientConfig{
			APIKey:  cfg.LLMAPIKey,
			Backend: genai.BackendGeminiAPI,
		})
		if err != nil {
			log.Printf("[LLM] Failed to create Gemini client: %v", err)
		} else {
			client.genaiClient = gClient
		}
	}

	return client
}

func (c *Client) AnalyzeTextStream(text string, onToken func(string)) ([]AnalysisResult, error) {
	if c.isMock {
		log.Println("[LLM] Mock Mode Enabled. Returning simulated result.")
		time.Sleep(1 * time.Second)

		// Simulate tokens
		mockTokens := []string{"{", "\"segments\": ", "[", "{", "\"text\": ", "\"Mock segment 1\"", ", \"speaker\": ", "\"Narrator\"", ", \"emotion\": ", "\"calm\"", "},", "{", "\"text\": ", "\"Mock dialogue\"", ", \"speaker\": ", "\"Hero\"", ", \"emotion\": ", "\"happy\"", "}]", "}"}
		for _, t := range mockTokens {
			if onToken != nil {
				onToken(t)
			}
			time.Sleep(50 * time.Millisecond)
		}

		return []AnalysisResult{
			{Text: "This is a mock narration segment.", Speaker: "Narrator", Emotion: "calm"},
			{Text: "This is a mock dialogue segment.", Speaker: "Hero", Emotion: "happy"},
			{Text: "Another mock narration.", Speaker: "Narrator", Emotion: "calm"},
		}, nil
	}

	if c.provider == "gemini" {
		return c.streamGemini(text, onToken)
	}

	// Rate Limiting: Cooldown
	c.mu.Lock()
	elapsed := time.Since(c.lastRequestTime)
	if elapsed < c.minInterval {
		wait := c.minInterval - elapsed
		log.Printf("[LLM] Rate Limit Cooldown: Waiting %v...\n", wait)
		time.Sleep(wait)
	}
	c.mu.Unlock()

	// Update lastRequestTime when we return
	defer func() {
		c.mu.Lock()
		c.lastRequestTime = time.Now()
		c.mu.Unlock()
	}()

	maxRetries := 3
	var lastErr error

	for attempt := 1; attempt <= maxRetries; attempt++ {
		log.Printf("[LLM] Sending Streaming request (Length: %d, Attempt: %d/%d)\n", len(text), attempt, maxRetries)

		var messages []openai.ChatCompletionMessage
		messages = []openai.ChatCompletionMessage{
			{Role: openai.ChatMessageRoleUser, Content: systemPrompt + "\n\n" + text},
		}

		req := openai.ChatCompletionRequest{
			Model:    c.model,
			Messages: messages,
			Stream:   true,
		}

		stream, err := c.api.CreateChatCompletionStream(context.Background(), req)
		if err != nil {
			var apiErr *openai.APIError
			if errors.As(err, &apiErr) {
				log.Printf("[LLM] Stream Creation API Error (Attempt %d): StatusCode=%d, Code=%s, Message=%s\n", attempt, apiErr.HTTPStatusCode, apiErr.Code, apiErr.Message)
			} else {
				log.Printf("[LLM] Stream Creation Error (Attempt %d): %v\n", attempt, err)
			}

			lastErr = err
			time.Sleep(1 * time.Second) // Basic backoff
			continue
		}

		var fullContent strings.Builder
		streamFailed := false
		var finishReason string

		for {
			response, err := stream.Recv()
			if errors.Is(err, io.EOF) {
				break
			}
			if err != nil {
				var apiErr *openai.APIError
				if errors.As(err, &apiErr) {
					log.Printf("[LLM] Stream Recv API Error (Attempt %d): StatusCode=%d, Code=%s, Message=%s\n", attempt, apiErr.HTTPStatusCode, apiErr.Code, apiErr.Message)
				} else {
					log.Printf("[LLM] Stream Recv Error (Attempt %d): %v\n", attempt, err)
				}
				streamFailed = true
				break
			}

			// Check if Choices array is empty to prevent index out of range panic
			if len(response.Choices) == 0 {
				log.Printf("[LLM] Warning: Received response with empty Choices array (Attempt %d)\n", attempt)
				continue
			}

			if len(response.Choices) > 0 {
				finishReason = string(response.Choices[0].FinishReason)
			}

			token := response.Choices[0].Delta.Content
			if token != "" {
				fullContent.WriteString(token)
				if onToken != nil {
					onToken(token) // Note: This might send partial tokens from failed attempts to UI, which is acceptable for now
				}
			}
		}
		stream.Close()
		log.Printf("[LLM] Stream Finished (Attempt %d). FinishReason: %s\n", attempt, finishReason)

		if streamFailed {
			lastErr = fmt.Errorf("stream interrupted")
			time.Sleep(1 * time.Second)
			continue
		}

		content := fullContent.String()
		log.Printf("[LLM] Full Response Accumulated for Parsing (Attempt %d).\n", attempt)

		// Robust JSON Extraction
		jsonContent, err := extractJSON(content)
		if err != nil {
			log.Printf("[LLM] JSON Extraction Error (Attempt %d): %v. Content: %s\n", attempt, err, content)
			lastErr = fmt.Errorf("failed to extract JSON: %v", err)
			time.Sleep(1 * time.Second)
			continue
		}

		// Helper struct for JSON Mode response wrapper
		type responseWrapper struct {
			Segments []AnalysisResult `json:"segments"`
		}

		var wrapper responseWrapper
		parseErr := json.Unmarshal([]byte(jsonContent), &wrapper)

		// If parse failed and we suspect truncation, try to repair *before* complaining
		if parseErr != nil && (finishReason == "content_filter" || finishReason == "length") {
			log.Printf("[LLM] Warning: Response truncated (FinishReason: %s). Attempting to repair JSON...\n", finishReason)

			// Simple repair: Try appending closing structures
			// The extractJSON likely gave us something starting with '{'
			// We tried to parse { "segments": [ ... ]
			// It might be cut off like { "segments": [ { ... }, { ...

			// strategy 1: Close array and object
			repairedContent := jsonContent + "]}"
			if errRepair := json.Unmarshal([]byte(repairedContent), &wrapper); errRepair == nil {
				log.Printf("[LLM] JSON repair successful. Proceeding with salvaged data.\n")
				return wrapper.Segments, nil
			}

			// strategy 2: maybe it was cut off inside a string or object?
			// tough to fix perfectly without a complex parser, but let's try a slightly more aggressive cut
			// Find last '},' and cut there, then close.
			if lastComma := strings.LastIndex(jsonContent, "},"); lastComma != -1 {
				repairedContent = jsonContent[:lastComma+1] + "]}"
				if errRepair := json.Unmarshal([]byte(repairedContent), &wrapper); errRepair == nil {
					log.Printf("[LLM] JSON repair successful (aggressive cut). Proceeding with salvaged data.\n")
					return wrapper.Segments, nil
				}
			}
		}

		if parseErr != nil {
			log.Printf("[LLM] JSON Parse Error (Attempt %d): %v. Content End: ...%s\n", attempt, parseErr, getLastNChars(jsonContent, 100))
			lastErr = fmt.Errorf("failed to parse LLM JSON: %v. Check log for content", parseErr)
			time.Sleep(1 * time.Second)
			continue
		}

		// Success!
		return wrapper.Segments, nil
	}

	return nil, fmt.Errorf("analysis failed after %d attempts. Last error: %v", maxRetries, lastErr)
}

func getLastNChars(s string, n int) string {
	if len(s) <= n {
		return s
	}
	return s[len(s)-n:]
}

func (c *Client) ListModels() ([]string, error) {
	if c.isMock {
		return []string{"mock-model-1", "mock-model-2"}, nil
	}

	c.mu.Lock()
	if c.provider == "gemini" {
		c.mu.Unlock()
		return []string{"gemini-3-flash-preview", "gemini-2.0-flash-exp", "gemini-1.5-flash", "gemini-1.5-pro"}, nil
	}
	// Rate limit check could be here if needed
	c.mu.Unlock()

	list, err := c.api.ListModels(context.Background())
	if err != nil {
		return nil, err
	}

	var models []string
	for _, m := range list.Models {
		models = append(models, m.ID)
	}
	return models, nil
}

// streamGemini handles the native Google Gemini API streaming using GenAI SDK
func (c *Client) streamGemini(text string, onToken func(string)) ([]AnalysisResult, error) {
	if c.genaiClient == nil {
		return nil, fmt.Errorf("gemini client not initialized")
	}

	// Rate Limiting
	c.mu.Lock()
	elapsed := time.Since(c.lastRequestTime)
	if elapsed < c.minInterval {
		time.Sleep(c.minInterval - elapsed)
	}
	c.mu.Unlock()

	defer func() {
		c.mu.Lock()
		c.lastRequestTime = time.Now()
		c.mu.Unlock()
	}()

	maxRetries := 3
	var lastErr error

	// Default to gemini-3-flash-preview if not specified
	model := c.model
	if model == "" {
		model = "gemini-3-flash-preview"
	}

	for attempt := 1; attempt <= maxRetries; attempt++ {
		log.Printf("[LLM] Sending Gemini SDK Streaming request (Length: %d, Attempt: %d/%d)\n", len(text), attempt, maxRetries)

		ctx := context.Background()

		// Use genai.Text helper to create contents
		contents := genai.Text(systemPrompt + "\n\n" + text)

		var fullContent strings.Builder
		streamFailed := false

		// Use range loop for iterator
		for resp, err := range c.genaiClient.Models.GenerateContentStream(ctx, model, contents, nil) {
			if err != nil {
				log.Printf("[LLM] SDK Stream Recv Error (Attempt %d): %v\n", attempt, err)
				streamFailed = true
				break
			}

			if resp != nil && len(resp.Candidates) > 0 {
				for _, part := range resp.Candidates[0].Content.Parts {
					// Using fmt.Sprint as genai.Text is a function, not a type we can assert to directly here without knowing the internal struct name.
					// The part is likely a struct that String()s nicely or we can refine later.
					// Access Text field directly. Assuming Part is a struct with Text field.
					txt := part.Text
					fullContent.WriteString(txt)
					if onToken != nil {
						onToken(txt)
					}
				}
			}
		}

		if streamFailed {
			lastErr = fmt.Errorf("stream interrupted")
			time.Sleep(1 * time.Second)
			continue
		}

		log.Printf("[LLM] Stream Finished (Attempt %d).\n", attempt)

		content := fullContent.String()
		log.Printf("[LLM] Full Response Accumulated for Parsing (Attempt %d).\n", attempt)

		// Robust JSON Extraction (Same as OpenAI)
		jsonContent, err := extractJSON(content)
		if err != nil {
			log.Printf("[LLM] JSON Extraction Error (Attempt %d): %v\n", attempt, err)
			lastErr = fmt.Errorf("failed to extract JSON: %v", err)
			time.Sleep(1 * time.Second)
			continue
		}

		type responseWrapper struct {
			Segments []AnalysisResult `json:"segments"`
		}

		var wrapper responseWrapper
		parseErr := json.Unmarshal([]byte(jsonContent), &wrapper)

		if parseErr != nil {
			log.Printf("[LLM] JSON Parse Error (Attempt %d): %v.\n", attempt, parseErr)

			// Try repair (same as existing)
			repairedContent := jsonContent + "]}"
			if errRepair := json.Unmarshal([]byte(repairedContent), &wrapper); errRepair == nil {
				log.Printf("[LLM] JSON repair successful.\n")
				return wrapper.Segments, nil
			}

			lastErr = fmt.Errorf("failed to parse LLM JSON: %v", parseErr)
			time.Sleep(1 * time.Second)
			continue
		}

		return wrapper.Segments, nil
	}

	return nil, fmt.Errorf("analysis failed after %d attempts. Last error: %v", maxRetries, lastErr)
}

// extractJSON attempts to find the first '{' and last '}' to extract the valid JSON object
func extractJSON(s string) (string, error) {
	start := strings.Index(s, "{")
	end := strings.LastIndex(s, "}")

	if start == -1 || end == -1 || start > end {
		return "", fmt.Errorf("no valid JSON object found in response")
	}

	return s[start : end+1], nil
}
